
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>EDGE PoC - Complete Reference Guide</title>
</head>
<body bgcolor="#FFFFFF" text="#000000" link="#000000" vlink="#000000" alink="#000000" style="font-family: monospace, courier; margin: 20px;">

<center>
<h1>EDGE PoC - COMPLETE REFERENCE GUIDE</h1>
<hr size="3" width="80%">
<p><b>Purpose:</b> Comprehensive reference combining developer runbook, API documentation, dataset understanding, and teacher guides for the EDGE (Educational Data-driven Guidance Engine) system.</p>
</center>

<hr>

<h2>TABLE OF CONTENTS</h2>
<ul>
    <li><a href="#section1">1. Developer Runbook & Quick Start</a>
        <ul>
            <li><a href="#section1-1">1.1 Quick Start</a></li>
            <li><a href="#section1-2">1.2 Project Layout</a></li>
            <li><a href="#section1-3">1.3 Environments & Dependencies</a></li>
            <li><a href="#section1-4">1.4 Data Generation, Training, Validation</a></li>
        </ul>
    </li>
    <li><a href="#section2">2. API Documentation & Endpoints</a>
        <ul>
            <li><a href="#section2-1">2.1 Core API Endpoints</a></li>
            <li><a href="#section2-2">2.2 Request/Response Examples</a></li>
            <li><a href="#section2-3">2.3 System Architecture</a></li>
        </ul>
    </li>
    <li><a href="#section3">3. Dataset Understanding & Structure</a>
        <ul>
            <li><a href="#section3-1">3.1 Dataset Overview</a></li>
            <li><a href="#section3-2">3.2 Data Dictionary</a></li>
            <li><a href="#section3-3">3.3 Student Lifecycle & Flow</a></li>
            <li><a href="#section3-4">3.4 Real-world Examples</a></li>
        </ul>
    </li>
    <li><a href="#section4">4. Infrastructure & Operations</a>
        <ul>
            <li><a href="#section4-1">4.1 Redis & Background Workers</a></li>
            <li><a href="#section4-2">4.2 Model Registry & Hot-swap</a></li>
            <li><a href="#section4-3">4.3 Docker & Deployment</a></li>
            <li><a href="#section4-4">4.4 Testing & CI</a></li>
        </ul>
    </li>
    <li><a href="#section5">5. Monitoring & Troubleshooting</a>
        <ul>
            <li><a href="#section5-1">5.1 Monitoring & Logs</a></li>
            <li><a href="#section5-2">5.2 Common Issues & Fixes</a></li>
            <li><a href="#section5-3">5.3 Maintenance Procedures</a></li>
        </ul>
    </li>
    <li><a href="#section6">6. Teacher Guide & Model Usage</a>
        <ul>
            <li><a href="#section6-1">6.1 Understanding Model Predictions</a></li>
            <li><a href="#section6-2">6.2 Decision Rules</a></li>
            <li><a href="#section6-3">6.3 Monitoring Over Time</a></li>
        </ul>
    </li>
    <li><a href="#section7">7. Reference & Utilities</a>
        <ul>
            <li><a href="#section7-1">7.1 Configuration Reference</a></li>
            <li><a href="#section7-2">7.2 Useful Commands</a></li>
        </ul>
    </li>
</ul>

<hr size="2">

<!-- ===================== SECTION 1 ===================== -->
<a name="section1"></a>
<h2>1. DEVELOPER RUNBOOK & QUICK START</h2>

<a name="section1-1"></a>
<h3>1.1 Quick Start</h3>
<p>Recommended: use Docker Compose for local development. Minimal steps:</p>
<pre>
# Build and start services (api, redis, worker)
docker compose up --build

# Open API docs (Swagger)
# http://localhost:8000/docs
</pre>
<p><b>Alternative (no Docker):</b> install Redis locally and run worker + API in separate shells.</p>

<a name="section1-2"></a>
<h3>1.2 Project Layout</h3>
<pre>
edge/
├── data/                      # dataset generation
├── models/                    # model algorithms utilities
├── policy/                    # bandit/policy logic
├── service/                   # FastAPI app & helpers
│    ├── app.py                # API
│    ├── state.py              # onboarding helpers
│    ├── redis_client.py       # redis helpers set_active_model, enqueue
│    └── ...
├── workers/                   # background workers (policy_worker.py)
├── train.py                   # training pipeline
├── validate.py                # evaluation & report generation
├── requirements.txt
├── Dockerfile
├── docker-compose.yml
└── artifacts/                 # generated models, CSVs, reports
</pre>

<a name="section1-3"></a>
<h3>1.3 Environments & Dependencies</h3>
<p>Python 3.8+ recommended. Use virtualenv or venv.</p>
<pre>
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
</pre>
<p>Added runtime dependencies: <code>redis</code>, <code>joblib</code>, <code>pytest</code>.</p>

<a name="section1-4"></a>
<h3>1.4 Data Generation, Training, Validation</h3>

<h4>Generate synthetic data</h4>
<pre>python -m edge.data.dataset_gen</pre>
<p>Generates <code>artifacts/items.csv</code>, <code>interactions_train.csv</code> and <code>interactions_test.csv</code>.</p>

<h4>Train models</h4>
<pre>python -m edge.train</pre>
<p><b>Outputs (under artifacts/):</b></p>
<ul>
    <li><code>irt_state.npz</code> - IRT theta, a, b, item_topics</li>
    <li><code>rf_miscon.joblib</code> - uncalibrated RF</li>
    <li><code>rf_miscon_cal.joblib</code> - calibrated RF (if present)</li>
    <li><code>gmm_miscon.joblib</code> - GMM misconception clusters</li>
</ul>

<h4>Validate</h4>
<pre>python -m edge.validate</pre>
<p>Generates metrics and <code>artifacts/validate_report.html</code> with calibration and per-topic confusion matrices.</p>

<hr>

<!-- ===================== SECTION 2 ===================== -->
<a name="section2"></a>
<h2>2. API DOCUMENTATION & ENDPOINTS</h2>

<a name="section2-1"></a>
<h3>2.1 Core API Endpoints</h3>
<p>Run server:</p>
<pre>uvicorn edge.service.app:app --reload --host 0.0.0.0 --port 8000</pre>
<p>Swagger: <code>/docs</code></p>

<p><b>Key endpoints:</b></p>
<ul>
    <li><b>POST /predict</b> - Input: LearnerState. Output: misconception_prob, policy_action, bandit_action, mastery_used.</li>
    <li><b>POST /recommend</b> - Input: learner_id, n. Returns recommended item ids.</li>
    <li><b>POST /feedback</b> - Input: feedback payload. Enqueues to Redis for async processing by worker.</li>
    <li><b>POST /reload_model</b> - Admin endpoint to hot-swap model from artifacts/active_model.json.</li>
</ul>

<a name="section2-2"></a>
<h3>2.2 Request/Response Examples</h3>

<h4>Health Check API</h4>
<p><b>Endpoint:</b> GET /health</p>
<p><b>Description:</b> Verifies if the system is running properly.</p>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tr bgcolor="#F0F0F0"><th>Request</th><th>Response</th></tr>
<tr>
<td><pre>GET /health</pre></td>
<td><pre>{ "status": "ok" }</pre></td>
</tr>
</table>

<h4>Predict API</h4>
<p><b>Endpoint:</b> POST /predict</p>
<p><b>Description:</b> Predicts the misconception probability and selects a bandit action based on the learner's profile.</p>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tr bgcolor="#F0F0F0"><th>Request</th><th>Response</th></tr>
<tr>
<td><pre>{
 "learner_id": 42,
 "concept_id": 2,
 "item_id": 10,
 "prior_mastery": 0.3,
 "response_time": 28.5,
 "attempts": 1,
 "hints": 0,
 "correct": 0,
 "fatigue_factor": 2,
 "text_quality": 0.7,
 "time_of_day": "morning",
 "device_type": "desktop"
}</pre></td>
<td><pre>{
 "misconception_prob": 0.35,
 "bandit_action": "remediate"
}</pre></td>
</tr>
</table>

<h4>Recommend API</h4>
<p><b>Endpoint:</b> POST /recommend</p>
<p><b>Description:</b> Recommends the next set of items (questions) for a learner based on their current profile and progress.</p>
<table border="1" cellpadding="5" cellspacing="0" width="100%">
<tr bgcolor="#F0F0F0"><th>Request</th><th>Response</th></tr>
<tr>
<td><pre>{
 "learner_id": 42,
 "n": 5
}</pre></td>
<td><pre>{
 "learner_id": 42,
 "recommended_items": [101, 102, 103, 104, 105]
}</pre></td>
</tr>
</table>

<hr>

<!-- ===================== SECTION 3 ===================== -->
<a name="section3"></a>
<h2>3. DATASET UNDERSTANDING & STRUCTURE</h2>

<a name="section3-1"></a>
<h3>3.1 Dataset Overview</h3>
<p>This section explains the data used by the EDGE system: how each file looks, what every column means, how the pieces fit together, and how the data is used the first time a learner shows up and on every subsequent visit.</p>

<table border="1" cellpadding="6" cellspacing="0" width="100%">
    <tr bgcolor="#F0F0F0"><th>File</th><th>Purpose</th><th>Who uses it</th></tr>
    <tr>
        <td><code>artifacts/items.csv</code></td>
        <td>Item bank (questions): topic, difficulty (b), discrimination (a), stem, distractors</td>
        <td>IRT (Evaluate), Recommender (Exercise), Counterfactual generator (Generate)</td>
    </tr>
    <tr>
        <td><code>artifacts/interactions_train.csv</code></td>
        <td>Past learner-item interactions for training (80%)</td>
        <td>IRT training (theta, a, b), RF misconception model, GMM (Diagnose)</td>
    </tr>
    <tr>
        <td><code>artifacts/interactions_test.csv</code></td>
        <td>Holdout interactions for validation (20%)</td>
        <td>Validation metrics: Accuracy, Precision/Recall/F1, ECE, Brier</td>
    </tr>
    <tr>
        <td><code>artifacts/irt_state.npz</code></td>
        <td>Trained IRT parameters: learner mastery matrix theta[i,t], item a[j], item b[j], item topics</td>
        <td>Fast scoring of P(correct), topic mastery lookup per learner</td>
    </tr>
    <tr>
        <td><code>artifacts/rf_miscon_cal.joblib</code></td>
        <td>Calibrated Random Forest to predict misconception probability</td>
        <td>POST /predict for misconception_prob</td>
    </tr>
    <tr>
        <td><code>artifacts/gmm_miscon.joblib</code></td>
        <td>Bayesian GMM to cluster patterns of mistakes (misconception types)</td>
        <td>POST /feedback to update misconception profiles</td>
    </tr>
    <tr>
        <td><code>artifacts/last_practice.npy</code></td>
        <td>Per-learner per-topic last practice time (for retention/spacing)</td>
        <td>Scheduler (Exercise)</td>
    </tr>
    <tr>
        <td><code>artifacts/bandit_state.pkl</code></td>
        <td>Bandit state for UCB1 or Thompson Sampling (counts/values/posteriors)</td>
        <td>Policy action selection (practice / remediate / challenge)</td>
    </tr>
</table>

<a name="section3-2"></a>
<h3>3.2 Data Dictionary</h3>

<h4>items.csv — Item Bank (Questions)</h4>
<p>Each row is one question. Columns:</p>
<table border="1" cellpadding="6" cellspacing="0" width="100%">
    <tr bgcolor="#F0F0F0"><th>Column</th><th>Type</th><th>Meaning</th><th>Used by</th></tr>
    <tr><td><b>item_id</b></td><td>int</td><td>Unique item identifier</td><td>All modules</td></tr>
    <tr><td><b>topic</b></td><td>int</td><td>Topic index (e.g., 0=Math Foundations, 1=Algebra, 2=Geometry, ...)</td><td>IRT, Recommender, Retention</td></tr>
    <tr><td><b>a</b></td><td>float</td><td>Item discrimination (slope): higher = better at separating abilities</td><td>IRT</td></tr>
    <tr><td><b>b</b></td><td>float</td><td>Item difficulty (location): higher = harder</td><td>IRT, Recommender (match difficulty)</td></tr>
    <tr><td><b>stem</b></td><td>str</td><td>Question text</td><td>Serving; counterfactual generation</td></tr>
    <tr><td><b>distractors</b></td><td>str (pipe-joined)</td><td>Wrong options, e.g. "A|B|C|D"</td><td>Serving; misconception analysis</td></tr>
</table>

<p><b>Mini sample (CSV rows):</b></p>
<pre>
item_id,topic,a,b,stem,distractors
101,1,1.1,-0.2,"Factor x^2 - 5x + 6","x=1|x=3|x=6|x=2"
102,1,0.9,0.3,"Solve 2x+3=9","x=2|x=4|x=3|x=6"
203,2,1.2,0.0,"Area of a rectangle 3x5","8|15|10|12"
304,3,0.8,0.7,"Derivative of x^2","x|2|2x|x^3"
405,0,1.0,-0.5,"Simplify 6/9","2/3|3/2|1/3|3/6"
</pre>

<h4>interactions_*.csv — Learner ↔ Item Events</h4>
<p>Each row is one learner attempt. Columns:</p>
<table border="1" cellpadding="6" cellspacing="0" width="100%">
    <tr bgcolor="#F0F0F0"><th>Column</th><th>Type</th><th>Meaning</th><th>Used by</th></tr>
    <tr><td><b>learner_id</b></td><td>int</td><td>Which learner attempted</td><td>All modules</td></tr>
    <tr><td><b>item_id</b></td><td>int</td><td>Which item was presented</td><td>Join with items.csv</td></tr>
    <tr><td><b>topic</b></td><td>int</td><td>Topic of that item (redundant for convenience)</td><td>IRT, retention</td></tr>
    <tr><td><b>a</b></td><td>float</td><td>Discrimination copied from items.csv</td><td>IRT training</td></tr>
    <tr><td><b>b</b></td><td>float</td><td>Difficulty copied from items.csv</td><td>IRT training</td></tr>
    <tr><td><b>response_time</b></td><td>float (sec)</td><td>How long they took</td><td>RF features; GMM features</td></tr>
    <tr><td><b>attempts</b></td><td>int</td><td>Retries within the item</td><td>RF; GMM</td></tr>
    <tr><td><b>hints</b></td><td>int</td><td>Hints used</td><td>RF; GMM</td></tr>
    <tr><td><b>correct</b></td><td>0/1</td><td>Outcome</td><td>IRT label; RF label; GMM subset (wrong only)</td></tr>
    <tr><td><b>misconception</b></td><td>0/1</td><td>Observed misconception flag (sim.)</td><td>RF label (or target proxy)</td></tr>
    <tr><td><b>time_of_day</b></td><td>str</td><td>morning/afternoon/evening</td><td>RF feature</td></tr>
    <tr><td><b>device_type</b></td><td>str</td><td>desktop/tablet/phone</td><td>RF feature</td></tr>
    <tr><td><b>text_quality</b></td><td>float 0..1</td><td>Answer/explanation quality (sim.)</td><td>RF feature</td></tr>
    <tr><td><b>fatigue_factor</b></td><td>int</td><td>Fatigue proxy (sim.)</td><td>RF feature</td></tr>
    <tr><td><b>chosen_distractor</b></td><td>int</td><td>Index of chosen wrong option if incorrect, else -1</td><td>GMM, misconception type analysis</td></tr>
</table>

<p><b>Mini sample (CSV rows):</b></p>
<pre>
learner_id,item_id,topic,a,b,response_time,attempts,hints,correct,misconception,time_of_day,device_type,text_quality,fatigue_factor,chosen_distractor
17,101,1,1.1,-0.2,42.3,1,0,1,0,evening,phone,0.62,3,-1
17,304,3,0.8,0.7,58.8,2,1,0,1,evening,phone,0.55,3,1
17,102,1,0.9,0.3,29.0,1,0,1,0,morning,desktop,0.71,2,-1
52,203,2,1.2,0.0,21.4,1,0,1,0,afternoon,tablet,0.80,1,-1
52,304,3,0.8,0.7,66.2,2,0,0,1,afternoon,tablet,0.48,2,2
</pre>

<h4>Column Cross-Walk (who uses what)</h4>
<table border="1" cellpadding="6" cellspacing="0" width="100%">
    <tr bgcolor="#F0F0F0"><th>Column</th><th>Evaluate (IRT)</th><th>Diagnose (GMM/RF)</th><th>Generate</th><th>Exercise (Policy)</th></tr>
    <tr><td><b>topic</b></td><td>Mastery per topic</td><td>Profile grouping</td><td>Condition prompts</td><td>Retention spacing</td></tr>
    <tr><td><b>a, b</b></td><td>Train 2PL</td><td>(features optional)</td><td>Pick near-difficulty</td><td>Pick mid difficulty</td></tr>
    <tr><td><b>correct</b></td><td>Label for IRT</td><td>Label/target for RF</td><td>Choose CF difficulty</td><td>Reward shaping</td></tr>
    <tr><td><b>response_time</b></td><td>—</td><td>RF/GMM feature</td><td>CF guardrails</td><td>Time penalty</td></tr>
    <tr><td><b>attempts, hints</b></td><td>—</td><td>RF/GMM feature</td><td>CF instruction</td><td>Priority inputs</td></tr>
    <tr><td><b>time_of_day, device_type</b></td><td>—</td><td>RF features</td><td>—</td><td>—</td></tr>
    <tr><td><b>chosen_distractor</b></td><td>—</td><td>GMM clustering</td><td>Steer CF options</td><td>—</td></tr>
</table>

<a name="section3-3"></a>
<h3>3.3 Student Lifecycle & Flow</h3>

<h4>First-time learner (cold start)</h4>
<ol>
    <li>Learner shows up (no history). System assigns default mastery (theta) per topic (usually ~0) or runs a short placement test.</li>
    <li>Recommender tries Calculus item 304 (b=0.7, harder). Asha answers incorrectly after 59s, uses 1 hint, picks distractor #1. GMM assigns high responsibility to cluster "confuses derivative with slope of secant". Misconception profile for Calculus increases.</li>
    <li>Policy sets action to <b>remediate</b>. The generator picks a counterfactual Calculus item targeting that misconception (e.g., explicit tangent vs. secant contrast). Next time Asha improves; reward credits the remediate action.</li>
    <li>Spacing: last_practice[17, topic] updates each time; topics not seen for a while get lower retention → higher priority.</li>
</ol>

<h4>Small Mini-Dataset (5 items × 3 students × 8 interactions)</h4>

<h5>A) items.csv (excerpt)</h5>
<pre>
item_id,topic,a,b,stem,distractors
11,1,1.0,0.0,"Solve 3x=12","x=3|x=4|x=6|x=9"
12,1,1.2,0.5,"Solve x^2=9","x=±3|x=3|x=9|x=±9"
21,2,0.9,-0.3,"Area of 4x7","21|28|18|11"
31,3,0.8,0.7,"Derivative of x^3","3x^2|x^2|x^3|3x"
41,0,1.1,-0.6,"Simplify 8/12","2/3|3/2|1/3|3/8"
</pre>

<h5>B) interactions_train.csv (excerpt)</h5>
<pre>
learner_id,item_id,topic,a,b,response_time,attempts,hints,correct,misconception,time_of_day,device_type,text_quality,fatigue_factor,chosen_distractor
7,11,1,1.0,0.0,24.1,1,0,1,0,morning,desktop,0.70,2,-1
7,31,3,0.8,0.7,61.0,2,1,0,1,evening,phone,0.52,3,2
18,21,2,0.9,-0.3,30.5,1,0,1,0,afternoon,tablet,0.77,1,-1
18,41,0,1.1,-0.6,19.2,1,0,1,0,afternoon,tablet,0.80,1,-1
55,31,3,0.8,0.7,70.3,2,1,0,1,evening,desktop,0.49,2,1
55,12,1,1.2,0.5,33.4,1,0,1,0,morning,desktop,0.68,2,-1
7,12,1,1.2,0.5,28.7,1,0,1,0,evening,phone,0.63,3,-1
55,21,2,0.9,-0.3,22.9,1,0,1,0,afternoon,desktop,0.75,1,-1
</pre>

<h5>C) How features map during prediction</h5>
<ul>
    <li>RF features: prior_mastery (from theta or baseline), response_time, attempts, hints, text_quality, fatigue_factor, time_of_day, device_type</li>
    <li>Label (during training): misconception (0/1) or a proxy</li>
    <li>GMM uses wrong attempts with features like response_time, log(time), hints, attempts to produce misconception responsibilities (soft cluster membership)</li>
</ul>

<h4>Frequently Asked: "Where does mastery come from?"</h4>
<ul>
    <li><b>Placement test:</b> short test on entry; fit theta via IRT from those items.</li>
    <li><b>Cold default:</b> set all topics to a small prior (e.g., 0) and update quickly with early answers.</li>
    <li><b>External test import:</b> convert external test scores to per-topic mastery and load into theta.</li>
</ul>

<h4>End-to-End Flow (One page summary)</h4>
<ol>
    <li><b>Item bank</b> defines topics + (a,b), stems and distractors.</li>
    <li><b>Interactions</b> accumulate (correct, time, hints, attempts, chosen_distractor, context).</li>
    <li><b>Evaluate (IRT):</b> learn theta (mastery per topic) and keep item parameters stable.</li>
    <li><b>Diagnose:</b> RF predicts misconception probability; GMM clusters error patterns.</li>
    <li><b>Generate:</b> heuristic or LLM-assisted counterfactuals targeting misconception types.</li>
    <li><b>Exercise:</b> priority index + bandit choose next action; retention decay boosts forgotten topics.</li>
    <li><b>Persist:</b> all states saved under artifacts/; reload next session.</li>
</ol>

<h4>What to look at when "it doesn't feel real"</h4>
<ul>
    <li>Distractors: make sure wrong options reflect real confusions (e.g., inverse vs reciprocal, tangent vs secant).</li>
    <li>Response times: longer on hard items; shorter on easy ones; fatigue increases time.</li>
    <li>Attempts & hints: correlate with difficulty and misconceptions.</li>
    <li>Chosen distractor: repeated patterns across many learners in a topic indicate genuine misconception clusters.</li>
</ul>


<a name="section3-4"></a>
<h3>3.4 Real-world Examples</h3>

<h4>Subjects & Topic IDs</h4>
<pre>
0=Math Foundations, 1=Algebra, 2=Geometry, 3=Calculus, 4=Statistics,
5=Physics Kinematics, 6=Chemistry Stoichiometry, 7=Biology Genetics,
8=English Grammar, 9=Reading Comprehension, (… up to 19 in PoC)
</pre>

<h4>Distractors (how they matter)</h4>
<ul>
    <li>Each item has 4 options in distractors. The correct answer is implied by the item content (PoC may store externally).</li>
    <li>When incorrect, chosen_distractor records which wrong option was picked (0..3). Consistent patterns across many learners in the same topic = a potential misconception cluster.</li>
</ul>

<h4>100-student cohort setup (example)</h4>
<ul>
    <li><b>Learners:</b> 100 students, IDs 0..99. Each has noisy initial mastery across topics (theta).</li>
    <li><b>Items:</b> ~800 items distributed across 20 topics, each with a ≈ 0.6..1.4; b ≈ −1.0..+1.0.</li>
    <li><b>Sessions:</b> Each student attempts a random sample of items over several days. Time-of-day and device vary.</li>
</ul>

<h4>Concrete walk-through (Student #17 "Asha")</h4>
<ol>
    <li>Asha starts with Algebra mastery near 0.2 (low). The system recommends item 102 (b=0.3, moderate). She answers correctly after 29s, no hints: Algebra mastery nudges up (theta[17,Algebra] ↑).</li>
    <li>Recommender picks item 304 (Calculus) next. Asha answers incorrectly after 58s, uses 1 hint, and selects distractor #1. Misconception cluster for Calculus increases.</li>
    <li>The system triggers remediation, selecting a counterfactual item targeting the misconception (e.g., contrast tangent vs. secant). Asha's mastery increases with remediation, and the reward credits the remediate action.</li>
    <li>Spaced repetition: The system schedules the next practice based on the retention timestamp and updates the learner's profile with the new practice date for the topic.</li>
</ol>

<h4>Real-time Feedback Integration</h4>
<p>The system can provide live feedback for each interaction, helping students like Asha recognize their mistakes and focus on understanding key concepts. It adapts based on student progress, the misconceptions they hold, and their mastery level for each topic.</p>

<h4>Frequently Asked: "How does the system adjust the learning path?"</h4>
<ul>
    <li><b>First-time learners:</b> The system begins with a cold start, assigning a default mastery score (usually low) and then adapts based on their performance.</li>
    <li><b>Subsequent interactions:</b> Learners progress through personalized learning paths based on past interactions, misconceptions detected, and overall progress.</li>
    <li><b>Real-time adjustments:</b> The bandit algorithm dynamically adjusts the learning path in real-time, optimizing for remediation, challenge, or review.</li>
</ul>

<hr>

<!-- ===================== SECTION 4 ===================== -->
<a name="section4"></a>
<h2>4. INFRASTRUCTURE & OPERATIONS</h2>

<a name="section4-1"></a>
<h3>4.1 Redis & Background Workers</h3>

<h4>Redis setup</h4>
<p>Default REDIS_URL is <code>redis://redis:6379/0</code>. Set with env var REDIS_URL for non-Docker setups.</p>
<pre>
# run locally if not using docker
redis-server
export REDIS_URL=redis://localhost:6379/0
</pre>

<h4>Keys used</h4>
<ul>
    <li><code>feedback_queue</code> (LIST): enqueued feedback JSON strings</li>
    <li><code>bandit:{learner_id}</code> (STRING): JSON with counts and values arrays</li>
    <li><code>learner:{learner_id}:last_practice</code> (STRING): JSON mapping topic->timestamp</li>
</ul>

<h4>Running the worker</h4>
<pre>python -u edge/workers/policy_worker.py</pre>
<p>Worker blocks on the queue and processes events, updating Redis.</p>

<a name="section4-2"></a>
<h3>4.2 Model Registry & Hot-swap</h3>
<p>Active model pointer: <code>artifacts/active_model.json</code>. Must contain <code>model_path</code>.</p>
<pre>
{"model_path":"artifacts/rf_miscon_cal.joblib","version":"v1","ts":1690000000}
</pre>

<p>To set active model programmatically:</p>
<pre>
from edge.service.redis_client import set_active_model
set_active_model({"model_path":"artifacts/rf_miscon_cal.joblib","version":"v1","ts":1690000000})
</pre>

<p>Then call:</p>
<pre>curl -X POST http://localhost:8000/reload_model</pre>

<p><b>Note:</b> for multi-process deployments, see runbook section on pub/sub and reload caveats.</p>

<a name="section4-3"></a>
<h3>4.3 Docker & Deployment</h3>
<pre>
# Build and run services (api, worker, redis)
docker compose up --build
</pre>
<p>Use <code>docker compose down</code> to stop. Access API on port 8000.</p>

<a name="section4-4"></a>
<h3>4.4 Testing & CI</h3>
<p>Unit tests included (pytest). Example:</p>
<pre>pytest -q</pre>

<p><b>CI recommendations:</b></p>
<ul>
    <li>Run tests inside ephemeral Redis (testcontainers) or start Redis in CI job step.</li>
    <li>Run train & validate as long-running integration check (optional).</li>
    <li>Linting and type checks (flake8, mypy).</li>
</ul>

<hr>

<!-- ===================== SECTION 5 ===================== -->
<a name="section5"></a>
<h2>5. MONITORING & TROUBLESHOOTING</h2>

<a name="section5-1"></a>
<h3>5.1 Monitoring & Logs</h3>

<h4>Logs</h4>
<p>Primary logs: API console, worker console, Redis. Audit files: <code>artifacts/rewards.jsonl</code>, <code>artifacts/*.log</code>.</p>

<h4>Metrics to track</h4>
<ul>
    <li>queue length: <code>LLEN feedback_queue</code></li>
    <li>prediction latency (p95)</li>
    <li>ECE and Brier (calibration drift)</li>
    <li>worker throughput events/min</li>
</ul>

<a name="section5-2"></a>
<h3>5.2 Common Issues & Fixes</h3>
<ul>
    <li><b>Missing mastery</b>: Ensure <code>irt_state.npz</code> exists; run training if absent.</li>
    <li><b>Model not found on reload</b>: Verify <code>artifacts/active_model.json</code> points to correct path and file exists.</li>
    <li><b>Redis connection refused</b>: Check REDIS_URL and ensure Redis is running.</li>
</ul>

<a name="section5-3"></a>
<h3>5.3 Maintenance Procedures</h3>
<ol>
    <li>Backup artifacts and Redis snapshots regularly.</li>
    <li>When promoting a new model: run training → validate → push to artifacts with versioned name → set active_model.json → reload.</li>
    <li>For zero-downtime: update K8s Deployment with new image or implement pub/sub reload across processes.</li>
</ol>

<hr>

<!-- ===================== SECTION 6 ===================== -->
<a name="section6"></a>
<h2>6. TEACHER GUIDE & MODEL USAGE</h2>

<a name="section6-1"></a>
<h3>6.1 Understanding Model Predictions</h3>
<p>The EDGE system predicts the probability that a learner is showing a misconception. These probabilities are reliable (calibrated) and can be used to guide instruction.</p>

<a name="section6-2"></a>
<h3>6.2 Decision Rules</h3>
<ul>
    <li><b>If probability ≥ 0.70:</b> The learner is very likely struggling. Provide <b>immediate targeted remediation</b> (explain the concept, show worked examples).</li>
    <li><b>If 0.40 ≤ probability < 0.70:</b> The learner may be at risk. Schedule <b>practice in the next 24 hours</b> (spaced repetition helps memory).</li>
    <li><b>If probability < 0.40:</b> The learner is likely ready. Offer a <b>challenge question</b> or move to the next topic.</li>
</ul>

<h4>Why not always 0.5?</h4>
<p>Teachers can adjust thresholds:</p>
<ul>
    <li><b>Lower threshold (0.10–0.30):</b> Catch all struggling learners (high recall), but risk over-remediating confident ones.</li>
    <li><b>Higher threshold (0.60+):</b> Intervene only when very sure (high precision), but some struggling learners may be missed.</li>
</ul>

<a name="section6-3"></a>
<h3>6.3 Monitoring Over Time</h3>
<p>If you notice the model's predictions no longer match reality (students flagged as struggling who are not, or vice versa), recalibration or retraining may be needed. The system tracks metrics like <b>ECE</b> and <b>Brier score</b> to monitor this.</p>

<hr>

<!-- ===================== SECTION 7 ===================== -->
<a name="section7"></a>
<h2>7. REFERENCE & UTILITIES</h2>

<a name="section7-1"></a>
<h3>7.1 Configuration Reference</h3>

<h4>Key configuration (ENV)</h4>
<pre>
REDIS_URL=redis://redis:6379/0
EDGE_THRESH_REMEDIATE=0.70
EDGE_THRESH_PRACTICE=0.40
EDGE_UCB_EXPLORATION_C=2.0
EDGE_BANDIT_KIND=ucb
</pre>

<h4>Environment Variables</h4>
<ul>
    <li><b>EDGE_THRESH_REMEDIATE:</b> float (default 0.70) — threshold above which remediation is recommended.</li>
    <li><b>EDGE_THRESH_PRACTICE:</b> float (default 0.40) — threshold above which practice is scheduled.</li>
    <li><b>EDGE_UCB_EXPLORATION_C:</b> float — UCB exploration constant (if using UCB bandit).</li>
    <li><b>EDGE_BANDIT_KIND:</b> string — 'ucb' or 'ts' to select bandit type.</li>
</ul>

<h4>Files and artifacts</h4>
<ul>
    <li><b>artifacts/irt_state.npz:</b> contains arrays theta, a, b, item_topics.</li>
    <li><b>artifacts/last_practice.npy:</b> per-learner per-topic last practice times.</li>
    <li><b>artifacts/rf_miscon_cal.joblib:</b> calibrated RF classifier.</li>
    <li><b>artifacts/gmm_miscon.joblib:</b> GMM misconception clusters.</li>
    <li><b>artifacts/learners.json:</b> registry of seen learner IDs.</li>
</ul>

<h4>Tuning Parameters</h4>

<h5>Data Generation (edge/data/dataset_gen.py)</h5>
<ul>
    <li><b>Scale Controls:</b> n_learners, n_items, n_topics, n_samples: control scale.</li>
    <li><b>Response Dynamics:</b> response_time mean/variance (fatigue, device effects).</li>
    <li><b>Misconception Modeling:</b> n_miscon (mixture components) and Dirichlet prior for learner profiles.</li>
</ul>

<h5>IRT Evaluate (2PL) (edge/models/irt.py)</h5>
<ul>
    <li><b>lr</b> (learning rate): start 0.01–0.05.</li>
    <li><b>epochs:</b> 2–5 for synthetic; >10 for real data.</li>
    <li><b>reg:</b> L2 regularization 1e-4…1e-3 to stabilize a/b/theta.</li>
    <li><b>Batch size:</b> 4k–16k for speed vs. stability.</li>
</ul>

<h5>Misconception Diagnose (Bayesian GMM)</h5>
<ul>
    <li><b>n_components:</b> try 4–12. Validate with BIC/AIC and interpretability.</li>
    <li><b>Feature matrix:</b> Add distractor embeddings, per-distractor error rates, or semantic features.</li>
</ul>

<h5>Supervised RF Misconception Model</h5>
<ul>
    <li><b>n_estimators:</b> 100–300; increase for noisy labels.</li>
    <li>Include IRT mastery, time, attempts, hints, fatigue, text_quality, device/time-of-day.</li>
    <li>Consider calibrated models: Platt/Isotonic (sklearn CalibratedClassifierCV).</li>
</ul>

<h5>Exercise Scheduling (edge/policy/scheduler.py)</h5>
<ul>
    <li><b>Priority index:</b> Weights for mastery, retention decay (lambda), pace, misconception penalty.</li>
    <li><b>Bandit (UCB1):</b> UCB exploration strength. Increase to explore more.</li>
    <li><b>Optional:</b> Replace UCB1 with Thompson Sampling or add DQN for stateful policies.</li>
</ul>

<h5>FastAPI Service (edge/service/app.py)</h5>
<ul>
    <li>Add rate limiting (e.g., behind API gateway).</li>
    <li><b>Logging:</b> add structured logs and request IDs.</li>
    <li>Batch scoring endpoint for high-throughput use.</li>
    <li>Add authentication if exposed externally.</li>
</ul>

<h5>LLM Counterfactuals (edge/models/llm_adapter.py)</h5>
<ul>
    <li>Set OPENAI_API_KEY and OPENAI_MODEL (e.g., gpt-4o-mini).</li>
    <li><b>Tune temperature</b> for diversity (0.2–0.7).</li>
    <li><b>Validate outputs:</b> Parseable JSON, plausibility checks.</li>
</ul>

<h5>Validation & Monitoring (edge/validate.py)</h5>
<ul>
    <li><b>Track Accuracy, Precision/Recall/F1, ECE, Brier.</b></li>
    <li>Add reliability diagrams; retrain or calibrate if ECE high.</li>
    <li><b>Online:</b> Track bandit reward over time; guardrails for regret spikes.</li>
</ul>

<h5>Production Tweaks Available</h5>
<ul>
    <li>Increase n_samples and epochs to stabilize IRT and RF.</li>
    <li>Use CalibratedClassifierCV for probability calibration.</li>
    <li>Add feature store & schema validation to avoid training-serving skew.</li>
    <li>Add retries/timeouts around LLM calls.</li>
</ul>

<a name="section7-2"></a>
<h3>7.2 Useful Commands</h3>

<h4>Data dictionary (short)</h4>
<pre>
items.csv: item_id, topic, a, b, stem, distractors
interactions_*.csv: learner_id, item_id, topic, response_time, attempts, hints, correct, misconception, time_of_day, device_type, text_quality, fatigue_factor, chosen_distractor, mastery
</pre>

<h4>Useful commands</h4>
<pre>
# Inspect Redis queue length
redis-cli LLEN feedback_queue

# Show bandit state for learner 42
redis-cli GET "bandit:42"

# Reload model
curl -X POST http://localhost:8000/reload_model

# Generate data
python -m edge.data.dataset_gen

# Train models
python -m edge.train

# Validate models
python -m edge.validate

# Run tests
pytest -q

# Start API server
uvicorn edge.service.app:app --reload --host 0.0.0.0 --port 8000

# Start policy worker
python -u edge/workers/policy_worker.py

# Docker commands
docker compose up --build
docker compose down
</pre>

<hr>

<center>
<p><b>END OF DOCUMENT</b></p>
<p>EDGE PoC - Complete Reference Guide</p>
</center>

</body>
</html>